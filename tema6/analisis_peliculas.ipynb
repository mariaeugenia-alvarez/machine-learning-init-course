{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales para análisis de sentimiento de reseñas de películas\n",
    "\n",
    "En este notebook exploraremos un dataset formado por reseñas de IMDB dejadas por usuarios sobre determinadas películas y el sentimiento que estas transmiten (positivo o negativo). Estaremos por tanto realizando un trabajo de clasificación binaria siendo nuestras dos clases reseñas positivas o negativas.\n",
    "\n",
    "Emplearemos los siguientes módulos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos cargando los datos a partir del archivo csv correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./data/movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que los datos se hayan cargado correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos trabajando con 50.000 filas, es decir, con 50.000 opiniones y dos variables, las opiniones y su sentimiento. Procedemos a realizar un brevísimo análisis exploratorio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio\n",
    "\n",
    "### Diccionario de datos\n",
    "\n",
    "En este caso contamos solo con dos variables:\n",
    "\n",
    "* La opinión sobre la película **review** que emite una opinión sobre la misma.\n",
    "* La intención del comentario **sentiment** en la que distinguimos solo entre opiniones positivas y negativas.\n",
    "\n",
    "Procedemos a continuación a explorar brevemente estas dos variables. Lo cierto es que sobre las opiniones solo podemos comprobar si hay huecos o no.\n",
    "\n",
    "### Datos perdidos\n",
    "\n",
    "Comprobamos si existen datos perdidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En principio los datos se encuentran completos.\n",
    "\n",
    "### Análisis de sentiment\n",
    "\n",
    "Estudiamos la variable **sentiment**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='sentiment', data=raw_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que nos enfrentamos a una clasificación binaria en la que las clases se encuentran totalmente equilibradas. Tenemos 25000 reseñas positivas y 25000 reseñas negativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí tenemos nuestro análisis exploratorio. Por tratarse de dato no estructurado y una variable etiqueta realmente no se puede profundizar mucho más. Procedemos a continuación al preprocesado de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de variables\n",
    "\n",
    "El análisis de sentimiento requiere un preprocesamiento bastante específico:\n",
    "\n",
    "* Respecto a los textos debemos eliminar las palabras más comunes que no aportan información (conjunciones, preposiciones, pronombres) y tokenizar (convertir nuestras palabras en números).\n",
    "\n",
    "* Respecto a la etiqueta (positiva o negativo) debemos convertirla en una etiqueta numérica:\n",
    "\n",
    "### Codificando la etiqueta\n",
    "\n",
    "Codificaremos los positivos como 1 y los negativos como 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas = np.array([1 if s == 'positive' else 0 for s in raw_data.sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenamos las reseñas en un array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = raw_data.review.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separamos en entrenamiento y validación\n",
    "\n",
    "Como ya estamos habituado separamos en dos conjuntos: uno de entrenamiento y uno de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    reviews,\n",
    "    etiquetas,\n",
    "    test_size=0.2,\n",
    "    stratify=etiquetas,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento del texto\n",
    "\n",
    "Comenzamos tokenizando nuestras reseñas. Recordemos que las redes neuronales solo admiten vectores numéricos como entrada por lo que es necesario convertir nuestras palabras en números y nuestras frases en vectores para poder entrenar una red neuronal. Keras nos proporcioa ya un tokenizador que solo requiere indicarle el tamaño de vocabulario, es decir, cuántas palabras distintas lo van a formar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_VOCAB = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=SIZE_VOCAB)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función seq_pad que nos permite por una parte tokenizar las reseñas y por otra homogeneizar su longitud, es decir, hacer que todas las reseñas tengan el mismo número de palabras, para ello simplemente se fija un número máximo de palabras, si la reseña tiene más se corta y si tiene menos se rellena con 0's hasta alcanzar dicho número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_pad(raw_texts, tokenizer, max_seq_len=MAX_SEQ_LEN):\n",
    "    seq = tokenizer.texts_to_sequences(raw_texts)\n",
    "    pad_seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_seq_len, padding='post')\n",
    "    return pad_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo\n",
    "\n",
    "Podemos proceder ya a construir nuestra red neuronal. Comenzamos usando un Embedding que nos permite crear un espacio vectorial de palabras, posteriormente usamos una red recurrente que permite que la red tenga en cuenta el orden de las palabras para sacar conclusiones y posteriormente colocamos una capa con 16 neuronas y una última capa de 1 neurona. La última capa solo tiene una neurona porque para cada secuencia solo nos interesa un valor: la probabilidad de que la opinión sea positiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "MAX_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(input_dim=SIZE_VOCAB,output_dim=EMBED_DIM,input_length=MAX_SEQ_LEN))\n",
    "model.add(layers.SimpleRNN(12))\n",
    "model.add(layers.Dense(16))\n",
    "model.add(layers.Dense(1))  # Una sola neurona pues nos interesa un solo valor\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez construido el modelo lo compilamos eligiendo nuestro optimizador, la función de pérdida y la métrica adecuada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adamax\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hecho esto procedemos al entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=seq_pad(X_train, tokenizer),\n",
    "    y=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(seq_pad(X_val, tokenizer), y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos nuestro modelo. Definimos una función que nos permita realizar predicciones a partir del mismo. Para ello definimos la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(reviews, model):\n",
    "    pred = model.predict(seq_pad(reviews, tokenizer))  #tokenizamos la frase y predecimos la probabilidad\n",
    "    pred_senti = ['positive' if p >= 0.5 else 'negative' for p in pred] # redondeamos a positiva si la probabilidad es 0.5 o más\n",
    "\n",
    "    for tr, s in zip(reviews, pred_senti):\n",
    "        print(tr, '-->', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comenzar con algunas reseñas bastante sencillas y observamos que acierta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevas_reviews = ['I totally loved the film', 'Titanic was ok but the ending ruined it', 'It was boring as hell', 'Terrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(nuevas_reviews, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como gestiona algunas más complicadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_complicadas = ['Thanks for wasting two hours of my life', 'I laughed a lot. It was actually a drama.', 'This movie is so good that words cannot express how amazing it is.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(reviews_complicadas, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un principio atina bastante aunque evidentemente sigue teniendo errores pues solo acierta un 85% en validación. Te propongo que intentes engañarlo tu mismo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podeís probar a introducir vuestras propias reviews y visualizar los resultados de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(['Prueba a engañarle'], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto llegamos al final del notebook en el que con apenas 30 celdas hemos construido una red neuronal capaz de interpretar las reseñas de personas con una precisión bastante alta. Si intentáis jugar con los parámetros de la red aumentando las neuronas y la venta quizá esta precisión sería incluso mejorable. No dudéis en interactuar con ella."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
